import React, { useRef, useEffect, useState } from 'react';
import faceRecognitionService from '../services/faceRecognitionService';

/**
 * Composant de capture faciale pour le pointage
 * Utilise l'API Naratech pour la reconnaissance faciale
 */
const AttendanceFaceCapture = ({ 
  mode = 'clockin', // 'clockin' ou 'clockout'
  onRecognitionSuccess,
  onRecognitionError,
  onClose
}) => {
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const streamRef = useRef(null);
  
  const [isStreaming, setIsStreaming] = useState(false);
  const [isCapturing, setIsCapturing] = useState(false);
  const [isRecognizing, setIsRecognizing] = useState(false);
  const [error, setError] = useState(null);
  const [capturedImage, setCapturedImage] = useState(null);
  const [recognitionResult, setRecognitionResult] = useState(null);
  const [apiStatus, setApiStatus] = useState(null);

  // V√©rifier l'√©tat de l'API au montage
  useEffect(() => {
    const checkApiHealth = async () => {
      try {
        const health = await faceRecognitionService.checkHealth();
        console.log('üîç Health check result:', health);
        setApiStatus(health);
        
        // V√©rifier si l'API est op√©rationnelle
        if (health.status === 'ok' || health.status === 'operational') {
          // API op√©rationnelle
          console.log('‚úÖ API Naratech op√©rationnelle');
          setError(null);
        } else if (health.status === 'error') {
          // API en erreur mais on peut quand m√™me essayer
          console.warn('‚ö†Ô∏è API Naratech en erreur:', health.error || health);
          setError(null); // Ne pas bloquer, on peut quand m√™me essayer
        } else {
          // Statut inconnu, consid√©rer comme op√©rationnel si on a des donn√©es
          console.log('‚ÑπÔ∏è Statut API inconnu, mais donn√©es pr√©sentes:', health);
          setError(null);
        }
      } catch (err) {
        console.error('‚ùå Erreur lors de la v√©rification de l\'API:', err);
        // Ne pas bloquer l'interface, juste afficher un avertissement
        setApiStatus({ status: 'error', error: err.message });
        setError(null); // Permettre quand m√™me l'utilisation
      }
    };
    
    checkApiHealth();
  }, []);

  // D√©marrer la webcam
  const startCamera = async () => {
    console.log('üé• D√©marrage de la cam√©ra...');
    try {
      setError(null);
      
      // V√©rifier si l'API est disponible
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('L\'acc√®s √† la cam√©ra n\'est pas support√© par ce navigateur.');
      }
      
      console.log('üé• Demande d\'acc√®s √† la cam√©ra...');
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 640 },
          height: { ideal: 480 },
          facingMode: 'user'
        }
      });
      
      console.log('‚úÖ Stream obtenu:', stream);
      
      // Attendre que l'√©l√©ment vid√©o soit disponible dans le DOM
      let retries = 0;
      const maxRetries = 10;
      
      while (!videoRef.current && retries < maxRetries) {
        console.log(`‚è≥ Attente de l'√©l√©ment vid√©o... (tentative ${retries + 1}/${maxRetries})`);
        await new Promise(resolve => setTimeout(resolve, 100));
        retries++;
      }
      
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        streamRef.current = stream;
        setIsStreaming(true);
        setError(null); // Effacer toute erreur pr√©c√©dente
        console.log('‚úÖ Cam√©ra d√©marr√©e avec succ√®s, isStreaming:', true);
        
        // Forcer la lecture de la vid√©o
        try {
          await videoRef.current.play();
          console.log('‚ñ∂Ô∏è Vid√©o en lecture');
        } catch (playError) {
          console.warn('‚ö†Ô∏è Erreur lors de la lecture automatique:', playError);
        }
      } else {
        console.error('‚ùå videoRef.current est toujours null apr√®s', maxRetries, 'tentatives');
        throw new Error('L\'√©l√©ment vid√©o n\'est pas disponible dans le DOM. Veuillez r√©essayer.');
      }
    } catch (err) {
      console.error('‚ùå Erreur lors du d√©marrage de la cam√©ra:', err);
      let errorMessage = 'Impossible d\'acc√©der √† la cam√©ra.';
      
      if (err.name === 'NotAllowedError' || err.name === 'PermissionDeniedError') {
        errorMessage = 'Permission refus√©e. Veuillez autoriser l\'acc√®s √† la cam√©ra dans les param√®tres du navigateur.';
      } else if (err.name === 'NotFoundError' || err.name === 'DevicesNotFoundError') {
        errorMessage = 'Aucune cam√©ra trouv√©e. V√©rifiez que votre cam√©ra est connect√©e.';
      } else if (err.name === 'NotReadableError' || err.name === 'TrackStartError') {
        errorMessage = 'La cam√©ra est d√©j√† utilis√©e par une autre application.';
      } else if (err.name === 'OverconstrainedError' || err.name === 'ConstraintNotSatisfiedError') {
        errorMessage = 'Les param√®tres de la cam√©ra ne sont pas support√©s.';
      } else {
        errorMessage = `Erreur d'acc√®s √† la cam√©ra: ${err.message || err.name}`;
      }
      
      setError(errorMessage);
    }
  };

  // Arr√™ter la webcam
  const stopCamera = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
    }
    setIsStreaming(false);
  };

  // Capturer une image depuis la vid√©o
  const captureImage = () => {
    if (!videoRef.current || !canvasRef.current) return null;

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const context = canvas.getContext('2d');

    // D√©finir les dimensions du canvas
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    // Dessiner l'image de la vid√©o sur le canvas
    context.drawImage(video, 0, 0, canvas.width, canvas.height);

    // Convertir en data URL
    const imageDataUrl = canvas.toDataURL('image/jpeg', 0.9);
    return imageDataUrl;
  };

  // Effectuer la reconnaissance faciale
  const recognizeFace = async () => {
    if (!isStreaming) {
      setError('Veuillez d\'abord d√©marrer la cam√©ra');
      return;
    }

    setIsCapturing(true);
    setIsRecognizing(true);
    setError(null);
    setRecognitionResult(null);

    try {
      // Capturer l'image
      const imageDataUrl = captureImage();
      if (!imageDataUrl) {
        throw new Error('Impossible de capturer l\'image');
      }

      setCapturedImage(imageDataUrl);

      // Effectuer la reconnaissance via l'API
      console.log('üîç Envoi de l\'image pour reconnaissance...');
      const result = await faceRecognitionService.recognizeFace(imageDataUrl, {
        returnQualityInfo: true,
        confidenceThreshold: 0.45 // Seuil MOYENNE: Pointage.jsx fera la validation finale (entreprise + ID)
      });

      console.log('üìä R√©sultat de la reconnaissance:', result);
      setRecognitionResult(result);

      // V√©rifier si une personne a √©t√© reconnue
      if (result.recognizedPerson) {
        const detection = result.recognizedPerson;
        
        // V√©rifier que la d√©tection est valide
        // Seuil MOYENNE: on passe la d√©tection au parent.
        // Pointage.jsx applique ensuite la validation m√©tier (employ√© dans l'entreprise, etc.)
        if (faceRecognitionService.isValidDetection(detection, 0.45)) {
          // Appeler le callback de succ√®s
          if (onRecognitionSuccess) {
            onRecognitionSuccess({
              personName: detection.name,
              confidence: detection.confidence_level,
              similarity: detection.similarity,
              qualityScore: detection.quality_score,
              image: imageDataUrl,
              processingTime: result.processingTime
            });
          }
        } else {
          throw new Error('Confiance de reconnaissance trop faible');
        }
      } else if (result.detections && result.detections.length > 0) {
        // Visage d√©tect√© mais non reconnu
        const detection = result.detections[0];
        const confidenceLevel = detection.confidence_level || 'REJET√â';
        const similarity = detection.similarity || 0;
        
        // Message plus informatif selon le niveau de confiance
        let errorMessage;
        if (confidenceLevel === 'REJET√â' || similarity < 0.35) {
          errorMessage = `Visage d√©tect√© mais non reconnu. Votre photo n'est peut-√™tre pas enregistr√©e dans le syst√®me. Veuillez contacter l'administrateur pour enregistrer votre photo. (Confiance: ${confidenceLevel}, Similarit√©: ${(similarity * 100).toFixed(1)}%)`;
        } else if (similarity < 0.45) {
          errorMessage = `Visage d√©tect√© mais confiance trop faible (${(similarity * 100).toFixed(1)}%). Veuillez am√©liorer l'√©clairage et repositionner votre visage face √† la cam√©ra.`;
        } else {
          errorMessage = `Visage d√©tect√© mais non reconnu. Confiance: ${confidenceLevel} (${(similarity * 100).toFixed(1)}%)`;
        }
        
        throw new Error(errorMessage);
      } else {
        throw new Error('Aucun visage d√©tect√© dans l\'image. Veuillez vous positionner face √† la cam√©ra.');
      }
    } catch (err) {
      console.error('Erreur lors de la reconnaissance:', err);
      const errorMessage = err.message || 'Erreur lors de la reconnaissance faciale';
      setError(errorMessage);
      
      if (onRecognitionError) {
        onRecognitionError(err);
      }
    } finally {
      setIsCapturing(false);
      setIsRecognizing(false);
    }
  };

  // Nettoyer √† la fermeture
  useEffect(() => {
    return () => {
      stopCamera();
    };
  }, []);

  // R√©attacher le stream si l'√©l√©ment vid√©o devient disponible
  useEffect(() => {
    if (isStreaming && streamRef.current && videoRef.current && !videoRef.current.srcObject) {
      console.log('üîß R√©attachement du stream √† l\'√©l√©ment vid√©o');
      videoRef.current.srcObject = streamRef.current;
      videoRef.current.play().catch(err => {
        console.warn('‚ö†Ô∏è Erreur lors de la lecture automatique:', err);
      });
    }
  }, [isStreaming]);

  return (
    <div className="space-y-4">
      {/* Statut de l'API */}
      {apiStatus && (
        <div className={`p-3 rounded-lg text-sm ${
          (apiStatus.status === 'ok' || apiStatus.status === 'operational') 
            ? 'bg-green-50 text-green-700 border border-green-200' 
            : apiStatus.status === 'error'
            ? 'bg-yellow-50 text-yellow-700 border border-yellow-200'
            : 'bg-yellow-50 text-yellow-700 border border-yellow-200'
        }`}>
          {(apiStatus.status === 'ok' || apiStatus.status === 'operational') ? (
            <div className="flex items-center gap-2">
              <svg className="w-4 h-4" fill="currentColor" viewBox="0 0 20 20">
                <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm3.707-9.293a1 1 0 00-1.414-1.414L9 10.586 7.707 9.293a1 1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z" clipRule="evenodd" />
              </svg>
              <span>API connect√©e ({apiStatus.loadedPersons || apiStatus.loaded_persons || 0} personne(s) enregistr√©e(s))</span>
            </div>
          ) : (
            <div className="flex items-center gap-2">
              <svg className="w-4 h-4" fill="currentColor" viewBox="0 0 20 20">
                <path fillRule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clipRule="evenodd" />
              </svg>
              <span>
                {apiStatus.error 
                  ? `API: ${apiStatus.error}` 
                  : "V√©rification de l'API en cours... Vous pouvez quand m√™me utiliser la cam√©ra"}
              </span>
            </div>
          )}
        </div>
      )}

      {/* Zone de vid√©o */}
      <div className="relative bg-black rounded-lg overflow-hidden" style={{ aspectRatio: '4/3' }}>
        {/* Toujours rendre l'√©l√©ment vid√©o pour qu'il soit disponible dans le DOM */}
        <video
          ref={videoRef}
          autoPlay
          playsInline
          muted
          className={`w-full h-full object-cover ${!isStreaming ? 'hidden' : ''}`}
          onLoadedMetadata={() => {
            console.log('‚úÖ Vid√©o charg√©e, dimensions:', videoRef.current?.videoWidth, 'x', videoRef.current?.videoHeight);
          }}
          onPlay={() => {
            console.log('‚ñ∂Ô∏è Vid√©o en lecture');
          }}
          onError={(e) => {
            console.error('‚ùå Erreur vid√©o:', e);
          }}
        />
        
        {/* Message quand la cam√©ra n'est pas d√©marr√©e */}
        {!isStreaming && (
          <div className="absolute inset-0 flex items-center justify-center text-white">
            <div className="text-center">
              <svg className="w-16 h-16 mx-auto mb-4 opacity-50" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M15 10l4.553-2.276A1 1 0 0121 8.618v6.764a1 1 0 01-1.447.894L15 14M5 18h8a2 2 0 002-2V8a2 2 0 00-2-2H5a2 2 0 00-2 2v8a2 2 0 002 2z" />
              </svg>
              <p className="text-sm opacity-75">Cam√©ra non d√©marr√©e</p>
            </div>
          </div>
        )}
        
        {/* Image captur√©e en overlay */}
        {capturedImage && (
          <div className="absolute inset-0 bg-black bg-opacity-50 flex items-center justify-center">
            <img 
              src={capturedImage} 
              alt="Image captur√©e" 
              className="max-w-full max-h-full"
            />
          </div>
        )}
        
        {/* Canvas cach√© pour la capture */}
        <canvas ref={canvasRef} className="hidden" />
      </div>

      {/* Messages d'erreur */}
      {error && (
        <div className="p-3 bg-red-50 border border-red-200 rounded-lg text-red-700 text-sm">
          <div className="flex items-center gap-2">
            <svg className="w-4 h-4" fill="currentColor" viewBox="0 0 20 20">
              <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM8.707 7.293a1 1 0 00-1.414 1.414L8.586 10l-1.293 1.293a1 1 0 101.414 1.414L10 11.414l1.293 1.293a1 1 0 001.414-1.414L11.414 10l1.293-1.293a1 1 0 00-1.414-1.414L10 8.586 8.707 7.293z" clipRule="evenodd" />
            </svg>
            <span>{error}</span>
          </div>
        </div>
      )}

      {/* R√©sultat de la reconnaissance */}
      {recognitionResult && recognitionResult.recognizedPerson && (
        <div className="p-3 bg-green-50 border border-green-200 rounded-lg text-green-700 text-sm">
          <div className="flex items-center gap-2 mb-2">
            <svg className="w-4 h-4" fill="currentColor" viewBox="0 0 20 20">
              <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm3.707-9.293a1 1 0 00-1.414-1.414L9 10.586 7.707 9.293a1 1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z" clipRule="evenodd" />
            </svg>
            <span className="font-semibold">Personne reconnue !</span>
          </div>
          <div className="text-xs space-y-1">
            <p><strong>Nom:</strong> {recognitionResult.recognizedPerson.name}</p>
            <p><strong>Confiance:</strong> {recognitionResult.recognizedPerson.confidence_level}</p>
            <p><strong>Similarit√©:</strong> {(recognitionResult.recognizedPerson.similarity * 100).toFixed(1)}%</p>
            <p><strong>Temps de traitement:</strong> {recognitionResult.processingTime.toFixed(0)}ms</p>
          </div>
        </div>
      )}

      {/* Boutons de contr√¥le */}
      <div className="flex gap-3">
        {!isStreaming ? (
          <button
            onClick={(e) => {
              e.preventDefault();
              e.stopPropagation();
              console.log('üîµ Bouton "D√©marrer la cam√©ra" cliqu√©');
              startCamera();
            }}
            className="flex-1 px-4 py-2.5 bg-[#0389A6] text-white rounded-lg hover:bg-[#027A94] transition-colors font-instrument text-sm"
          >
            D√©marrer la cam√©ra
          </button>
        ) : (
          <>
            <button
              onClick={recognizeFace}
              disabled={isRecognizing || isCapturing}
              className="flex-1 px-4 py-2.5 bg-[#01A04E] text-white rounded-lg hover:bg-[#019A47] transition-colors font-instrument text-sm disabled:opacity-50 disabled:cursor-not-allowed"
            >
              {isRecognizing ? (
                <span className="flex items-center justify-center gap-2">
                  <svg className="animate-spin h-4 w-4" viewBox="0 0 24 24">
                    <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4" fill="none"/>
                    <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"/>
                  </svg>
                  Reconnaissance en cours...
                </span>
              ) : (
                `Reconna√Ætre (${mode === 'clockin' ? 'Entr√©e' : 'Sortie'})`
              )}
            </button>
            <button
              onClick={stopCamera}
              className="px-4 py-2.5 bg-gray-500 text-white rounded-lg hover:bg-gray-600 transition-colors font-instrument text-sm"
            >
              Arr√™ter
            </button>
          </>
        )}
        
        {onClose && (
          <button
            onClick={onClose}
            className="px-4 py-2.5 bg-gray-300 text-gray-700 rounded-lg hover:bg-gray-400 transition-colors font-instrument text-sm"
          >
            Fermer
          </button>
        )}
      </div>
    </div>
  );
};

export default AttendanceFaceCapture;
